[{"content":" When I was in New York City for the Quarks2Cosmos APS April Meeting this year, I was chatting with some friends about black hole evaporation, where black holes evaporate away due to quantum effects near their apparent horizon thereby violating unitarity in quantum mechanics. The phenomenon is at the heart of many developments in modern theoretical physics and was one of my primary inspirations to study the information loss problem earlier in my career. This process of evaporation is effectively encoded in the semi-classical Einstein equations $$ G _{ab} + \\Lambda g _{ab} = {8\\pi G _{\\rm N}}\\left\\langle{\\hat{T} _{ab}}\\right\\rangle _{\\psi} $$\nwhere classical gravity arising from a Lorentzian metric $g_{ab}$ is coupled to the expectation value of a quantum stress-energy operator $\\left\\langle{\\hat{T} _{ab}}\\right\\rangle$ acting on some predefined global quantum state $\\psi$. For certain geometries and quantum matter, the backreaction of quantum effects near the horizon causes the black hole surface area to shrink.\nThe connection between black hole evaporation and information loss comes from the Bekenstein-Hawking entropy formula $$ S _{\\rm BH} = \\frac{\\mathcal{A} _{\\mathcal{H}} }{4 G _{\\rm N}} $$ where $\\mathcal{A} _{\\mathcal{H}}$ is the surface area of the black hole with an event horizon $\\mathcal{H}$ and $S _{\\rm BH}$ is the thermal entropy required to construct such a black hole. Hence, black hole evaporation is identical to information loss if one interprets $S _{\\rm BH}$ as an information measure.1\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e But, as I was saying to my friends during the conference, black hole evaporation is not the only arena where information loss occurs. There is another; the Ricci Flow. Here, I discuss the Ricci flow, introduce the Fisher information, and describe the loss of Fisher information via Ricci flow. As a bonus, there are nice bounds that one can construct and I wrap things up by contrasting information loss via Ricci flow against information loss from black hole evaporation.\nThe Fisher Information # One of the early joys of Perelman\u0026rsquo;s work is that Ricci flow admits a gradient flow or variational reformulation. The setup is as follows, for closed Riemannian manifolds equipped with a metric and a scalar field $(\\Sigma,\\gamma,\\Phi)$, consider the Fisher information (or the CGHS model without additional matter in $d$ dimensions) given by $$ \\mathcal{F}_ \\Sigma = \\kappa\\int_\\Sigma (R+ \\gamma^{ab}\\partial_ a\\Phi\\partial_ b\\Phi)e^{-\\Phi}\\sqrt{\\gamma} d^dx,\\ \\ \\ \\ \\kappa = \\frac{1}{16\\pi G_ N} $$ One can embed $\\Sigma$ as an initial data hypersurface in a Lorentzian spacetime with metric $$ ds^2 = -N(x^a)dt^2 + \\gamma_{ab}(t) dx^a dx^b $$ thereby promoting the Fisher information stored in the initial data surface $\\Sigma = \\Sigma_ {t=0}$ to a time-dependent quantity $$ \\mathcal{F}(t) \\equiv \\mathcal{F}_ {\\Sigma_ t} =\\kappa \\int_ {\\Sigma_ t}(R_t + \\gamma_t^{ab}\\partial_ a\\Phi\\partial_ b\\Phi)e^{-\\Phi}\\sqrt{\\gamma_ t}d^d x. $$\nTo show that the Ricci flow emerges as a gradient flow of the Fisher information, we need to consider an extremization problem subject to the following constraint: $$ \\partial_ t\\tilde{\\mu}[\\Sigma] = \\frac{\\partial}{\\partial t}\\int_{\\Sigma_t}e^{-\\Phi }\\sqrt{\\gamma_t} d^d x = 0 $$ Now a quick calculation shows that the Fisher information has the first variation $$ \\partial_ t \\mathcal{F}(t) = -\\kappa\\int (R_ {ab} + \\nabla_ a\\nabla_ b \\Phi) \\partial_ t\\gamma^{ab} e^{-\\Phi}\\sqrt{\\gamma_ t} d^{d}x $$ From here, we can see that if we had a solution to the following flow $$ \\partial_ t \\gamma_{ab} = - 2\\ell (R_ {ab} + \\nabla_ a\\nabla_ b\\Phi)\\newline {\\partial_ t\\Phi} = {\\partial_ t \\sqrt{\\gamma}} $$ (which is just the dimensionally consistent Ricci flow in disguise, see Topping Section 6) subject to the constraint $\\partial_t \\tilde{\\mu} = 0$ then the Fisher information would evolve monotonically. Concretely we have the theorem\nTheorem: Monotonicity of Fisher Information # \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e If for a metric $\\gamma_t$ on a closed manifold $\\Sigma_ t$, there exists a constant function $\\omega = e^{-\\Phi}\\sqrt{\\gamma_ t}\u0026gt;0$ such that $(\\gamma_t,\\omega)$ solves the Ricci flow problem on an interval $(t_0,t_f)$, then the Fisher information is monotonically increasing along the flow $$ \\partial_ t \\mathcal{F}(t) = 2\\kappa \\ell \\int(R_ {ab} + \\nabla_ a\\nabla_ b\\Phi)^2e^{-\\Phi}\\sqrt{\\gamma_ t}d^d x \\ge 0. $$ Now, this is quite nice! It seems that there is a appropriate information measure which behaves nicely under the Ricci flow discussed above. In fact, one can do much better. It turns out that the growth of the (subtracted or normalized) classical Boltzmann-Shannon entropy provides the following bound on the Fisher information, $$ \\partial_ t \\mathcal{F} \\ge \\frac{2}{d\\ell} \\mathcal{F}^2. $$ This is a resource if one wants to construct sharper bounds on geometric quantities.\nThe Area Growth Bound # The proof techniques involved in utilizing Ricci flow to establish the monotonicity of the Fisher information (or the CGHS model) is essential prove a growth bound on ${\\rm Area}(\\Sigma_ t)$.\nThe essence of the argument boils down to the constraint imposed on the conformal volume measure $$ \\tilde{\\mu}[\\Sigma] = \\int_{\\Sigma_ t}e^{-\\Phi} \\sqrt{\\gamma_ t} d^dx $$ to formulate the Ricci flow as a gradient flow. Physically, the reason for the condition $\\partial_ t\\tilde{\\mu}[\\Sigma]=0$ on the conformal volume measure is introduce some control on how quickly the volume (or area) can increase under the flow. But essentially this can be leveraged to provide a bound on the growth of the actual volume measure on $\\Sigma_ t$ defined by $$ {\\mathcal{A}[\\Sigma_ t] = \\int_ {\\Sigma_ t} \\sqrt{\\gamma_ t}d^d x} $$ This is the main player in the following analysis and the remaining calculation is to ascertain a bound on its growth. Henceforth, I refer to this quantity as the area of $\\Sigma$.\nThe Einstein-Hilbert Action and Area Growth # We can directly check the variation of $\\mathcal{A}[\\Sigma_ t]$, we have $$ \\partial_t \\mathcal{A}[\\Sigma_ t] = \\int_{\\Sigma_ t}\\partial_t \\sqrt{\\gamma_ t}\\ d^dx. $$ A common identity in Riemannian (and even Lorentzian) geometry is that variations in the measure can be recast in terms of the metric $$ \\partial_ t \\sqrt{\\gamma_ t}= \\frac{\\sqrt{\\gamma_t}}{2}\\gamma^{ab}(t)\\partial_ t\\gamma_ {ab}(t) $$ At this stage, we can use up the result from the analysis of the Fisher information, namely that $$ \\gamma^{ab}\\partial_ t\\gamma_ {ab} = -2\\ell (R + \\Delta\\Phi ) $$ then the right hand size becomes $$ \\partial_ t\\sqrt{\\gamma_ t} = -\\ell(R + \\Delta \\Phi)\\sqrt{\\gamma} $$\nHence, applying these formulas, the growth of the area is $$ \\begin{align*} \\partial_ t\\mathcal{A}[\\Sigma_ t] \u0026amp;= - \\ell \\int_ {\\Sigma_ t}(R+\\Delta \\Phi)\\sqrt{\\gamma_ t}d^dx\\newline \u0026amp;= -\\ell\\int_ {\\Sigma_ t}R \\sqrt{\\gamma_ t}d^dx \\end{align*} $$ where I have integrated away $\\Delta \\Phi$ as there is no boundary.\nRemarkably, the preservation of the conformal volume measure along the Ricci flow gives us that the area growth of $\\Sigma$ is precisely the action for classical gravity, i.e., the Einstein-Hilbert action. Clearly, there are additional boundary contributions when $\\Sigma$ is not closed as I have been assuming throughout this article. However, for the manifolds under consideration thus far, we have the following theorem\nTheorem: The Boundedness of Area Growth # \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e If for a metric $\\gamma_ t$ on a closed manifold $\\Sigma_ t$, there exists a constant function $\\omega = e^{-\\Phi}\\sqrt{\\gamma_ t}\u0026gt;0$ such that the data $(\\gamma_{t=0},\\omega)$ solves the Ricci flow problem on an interval $(t_0,t_f)$, then the area growth is bounded above by $$ \\left|{\\partial_ t \\mathcal{A}[\\Sigma_ t]}\\right| = \\left|\\int_ {\\Sigma_ t}R \\sqrt{\\gamma_ t} d^dx\\right| \\le \\int_ {\\Sigma_ t} |R|\\sqrt{\\gamma_ t}d^d x \\le \\mathcal{R} \\mathcal{A}[\\Sigma_t] $$ where $\\mathcal{R}$ is defined by the maximum value of the absolute scalar curvature $|R|$ $$ \\mathcal{R} = \\max_{x \\in \\Sigma_ t}(|R(x)|). $$ Although this is the central result of this post, one cannot make further headway unless an explicit form or at least symmetry is provided for the metric $\\gamma_{ab}.$ Next I\u0026rsquo;ll briefly review some monotonicity bounds in the context of maximally symmetric spaces and ADM spaces.\nMaximally Symmetric Spaces # For maximally symmetric spaces, one has that $R_ {ab} = (2/d)\\Lambda \\gamma_ {ab}$ where $\\Lambda\\in\\mathbb{R}.$ For these manifolds, the area growth can be shown to satisfy the differential equation $$ \\partial_ t \\mathcal{A}[\\Sigma_ t] = - 2\\Lambda \\ell \\mathcal{A}[\\Sigma_ t] $$\nwhich has the general solution $$ \\mathcal{A}[\\Sigma_ t] = \\mathcal{A}[\\Sigma_0]\\exp(-2\\Lambda\\ell t). $$\nThe coefficient in the exponent gives us a measure on the growth rate of the area. Here, the length parameter is naturally set by the radius of the sphere $(\\Lambda\u0026gt;0)$, hyperbolic space $(\\Lambda\u0026lt;0)$, or the periodicity of the torus $(\\Lambda=0)$. For maximally symmetric spaces, the area growth saturates all inequalities, since $$ |\\partial_t\\mathcal{A}[\\Sigma_t]| = |2\\Lambda\\ell|\\mathcal{A}[\\Sigma_t] =\\mathcal{R}\\mathcal{A}[\\Sigma_t] $$\nLyapunov Exponents, and Entropy Production # In addition to being associated with the absolute scalar curvature, there is another natural interpretation afforded to the growth coefficient, namely that\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e For a maximally symmetric surface $\\Sigma_t$ with data $(\\gamma_t,\\Phi_t)$, $-2\\Lambda\\ell$ characterizes the rate of separation of two points on a surface $\\Sigma_{t_i}$ during the interval $(t_i,t_f)$, i.e., this quantity is the Lyapunov exponent $$ \\lambda_{\\rm Lyapunov} = -2\\Lambda\\ell. $$ In the context of dynamical systems, the Lyapunov spectrum is used to estimate the rate of entropy production. It is quite tempting to characterize this entropy production in the context of black hole information. Indeed, the Bekenstein-Hawking formula conjectures that the entropy of a black hole is proportional to the surface area of the black hole. Applying this conjecture here for a spherically symmetric space, one finds that a spherical shell of radius $\\ell$ with the initial metric $$ ds^2_ {\\Sigma} = \\ell ^2ds^2_{\\mathbb{S}^d},\\ \\ \\ \\ \\ \\ \\Lambda_{\\rm sph} = \\frac{(d-1)}{\\ell^2} $$ has an exponentially decreasing entropy $$ S(t) = \\frac{\\mathcal{A}[\\mathbb{S}_ t^d]}{4G_ N} = S_ {\\mathbb{S}^d} \\exp\\left(-\\frac{2(d-1)}{\\ell} t\\right). $$ In the language of dynamical systems, the spherical spaces have a negative Lyapunov exponent $\\lambda_{\\rm Lya.}\u0026lt;0$ and therefore are dissipative/shrink under the Ricci flow. Note that the entropy loss is quick at early times and slow at late times.\nEntropy Loss in Canonical Ricci Flow # In fact, independent of the calculation presented here, it is known that a sphere collapses linearly under the canonical Ricci flow in finite flow time (see Topping, Perelman, and Tao). For spheres, the canonical Ricci flow yields $$ \\begin{align*} ({\\rm Canonical\\ RF})\\ \\ \\ \\partial_t\\gamma_{ab} \u0026amp;= -2\\ell R_{ab} = -\\frac{4\\ell \\Lambda}{d} \\gamma^i_{ab} \\end{align*} $$ Solving this differential equation, it is clear to see that $$ \\begin{align*}\\gamma_{ab}(t) \u0026amp;= \\left(1-\\frac{4\\ell\\Lambda}{d}t\\right)\\gamma_{ab}^i\\newline \\implies S_{\\rm canon}(t) \u0026amp;= S_{\\mathbb{S}^d}\\left(1-\\frac{4\\ell\\Lambda}{d}t\\right)^{d/2} \\end{align*} $$ Unlike the Ricci flow with the conformal volume constraint, the collapse of the sphere is finite $$ t_{\\rm collapse} = \\frac{d}{4\\ell \\Lambda}. $$ In our case, the exponential collapse is due to the conformal volume constraint.\nComments on Black Hole Evaporation # This entropy loss due to Ricci flow for spheres is analogous to the entropy loss of a Schwarzschild black hole in the semiclassical regime. But one should be cautious when making this comparison.\nForemost, the Ricci flow is not relativistic since $1^{st}$ order in coordinate time. It is much more analogous to the Heat/Schroedinger equation as schemeatically one has $$ \\partial_t \\gamma_{ab} = f_{ab}(\\partial^2\\gamma,(\\partial\\gamma)^2) $$ whereas Hawking evaporation takes place on a relativistic arena.\nSecondly, given that $S_{\\rm Sch}(t) = \\mathcal{A}/4G_ N\\propto r_+^{d-1}$ and $M_ {\\rm Sch} \\propto r_ +^{d-2}$, and assuming that the black hole can be taken to be a perfect blackbody, one finds that the mass loss gives $$ \\frac{\\partial M}{\\partial t} \\sim -\\frac{1}{M^2} \\implies S_{\\rm Sch}(t)\\propto S_{\\mathbb{S}^d}\\left(1-\\frac{t}{t^\\prime_{\\rm collapse}}\\right)^{(d-1)/{3(d-2)}}. $$ This is certainly qualitatively different to the entropy loss due to Ricci flow when the conformal volume is preserved. Whereas the Hawking evaporation picture seems to predict total entropy loss in finite time, the Ricci flow subject to a conformal volume constraint leaves an infinitely slow decaying remnant.\nHawking radiation is expected to evaporate black holes in spacetimes with any asymptotics. However, the conformal volume preserving Ricci flow generates entropy $\\Lambda\u0026lt;0$ or is unchanged in flat space $\\Lambda=0$ which is in tension with what we know from quantum field theory in curved spacetime with Minkowski asymptotics.\nHowever, it is interesting to note the canonical Ricci flow is much more similar to Hawking evaporation, which raises the question\nFor what Ricci flow does one obtain the same entropy loss profile?\nFor a long time, there was a great confusion about whether this entropy is really related to entropy in information theory, but this is a story for another time.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"5 July 2022","permalink":"/posts/2022-07-05-information-loss-via-ricci-flow/","section":"Posts","summary":"When I was in New York City for the Quarks2Cosmos APS April Meeting this year, I was chatting with some friends about black hole evaporation, where black holes evaporate away due to quantum effects near their apparent horizon thereby violating unitarity in quantum mechanics.","title":"Information Loss via Ricci Flow"},{"content":" I have a soft spot for studying chaotic systems. In the prehistoric times of 2013 BC (Before Covid), Wenbo Tang and I wrote on a paper on the influcene of turbulence, such as eddies and streams, in the chaotic mixing of nutrient-phytoplankton-zooplankton systems. Recently, I\u0026rsquo;ve become interested in applications of machine learning and neural networks in the domain of physics and I came across this fun Quanta article on\nhow well can chaotic systems be modeled with physics inspired neural networks?\nI wanted to study this question for a simple chaotic problem, the Driven and Damped Harmonic Oscillator, implementing the Physics Informed Neural Networks architecure I read about on Ben\u0026rsquo;s blog. This is a continuation of my first post but applied to dynamical systems displaying chaotic structures in Python.\nConditions on Chaos # Most classical (or even quantum) systems studied by a budding theoretical physicist in an academic setting are often idealized toy models with exact analytic solutions. In reality, however, there are limitations to the predictability of any system. The weather, the stock market, or whether or not you will beat traffic to make it to work on time are all example of real-life systems which depend on conditions which you have no control over.\nThese are examples of chaotic systems. At first glance, it might seem as though chaotic systems are unpredictable or indeterministic, but this is not true. Even chaotic systems have underlying patterns. What makes a system chaotic is1\nbehavior which is non-repetitive, or aperiodic, and the system is highly sensitive to starting, or initial, conditions. If both requirements are met, then we say the system is deterministically chaotic. For example, let\u0026rsquo;s study the damped harmonic oscillator with and without a driving force term. Mathematically, the objective is to solve the system\n$$ \\begin{equation} \\ddot{\\chi}(t) + 2\\delta,\\dot{\\chi}(t) + \\omega^2, \\chi(t) = a \\sin(\\omega_d t) \\end{equation} $$\nsubject to some initial data $(\\chi(0)=\\xi,\\dot{\\chi}(0)=\\zeta)$ where $\\delta = \\mu/2m$, $\\omega^2 = k/m$, $(a,\\omega_d)$ are the acceleration and frequency of the driving. Of course, there are exact solutions (which can be found in the details below) but it is more instructive to study a representative set of solutions numerically.\nExact Solution # For no driving $(a=0)$, Mathematica gives the solution $$ \\begin{equation*} \\chi(t) = e^{-\\delta t} \\left(\\frac{(\\delta \\xi +\\zeta ) \\sin \\left(t \\sqrt{\\omega^2-\\delta ^2}\\right)}{\\sqrt{\\omega^2-\\delta^2}}+\\xi \\cos \\left(t \\sqrt{\\omega^2 - \\delta ^2}\\right)\\right). \\end{equation*} $$ With driving arbitrary, the solution takes the form $$ \\begin{align*} \\chi(t) \u0026amp;= e^{-t\\delta}\\left( c_1 e^{-t \\sqrt{\\delta ^2-\\omega ^2}}+c_2 e^{t \\sqrt{\\delta ^2-\\omega ^2}} \\right) + \\frac{a \\left(\\omega ^2-f^2\\right) \\cos (f t)+2 a \\delta f \\sin (f t)}{f^4+f^2 \\left(4 \\delta ^2-2 \\omega ^2\\right)+\\omega ^4} \\end{align*} $$ where $c_i$ are determined by using the initial data. Note that for oscillatory behavior, we need that $\\delta ^2-\\omega ^2\u0026lt;0$.\nCode Solutions # Here I document how to use Python to study the harmonic oscillator with a damping and driving term.\n# Import Packages import numpy as np import matplotlib.pyplot as plt # Required to plot in Jupyter Lab %matplotlib inline from PIL import Image # Import ODE Solver and Set TeX Presets in plots from scipy.integrate import odeint plt.rcParams[\u0026#39;text.usetex\u0026#39;] = True fs = 16 # set fontsize # Code to solve the Driven+Damped Harmonic Oscillator # Inputs: Vector, Initial conditions, and Parameters # Setting Initial Conditions (starting equ) xi = 1 zeta = 0 V0 = [xi, zeta] # Set Parameters a = 20 omegad = 17 # Construct the time domain Nt = 1000 t = np.linspace(0., 3., Nt) # Construct a system of coupled 1st order ODEs # DHO returns the differential equations for # \\phi = \\dot{chi}, and \\dot{\\phi} = -2*\\delta*\\phi - \\omega^2\\chi def DHO(V,t,delta,omega): return [V[1], -2*delta*V[1] - (omega**2)*V[0]] def DHOwD(V,t,delta,omega,a,omegad): return [V[1], -2*delta*V[1] - (omega**2)*V[0] + a*np.sin(omegad * t)] # The solution to the differential equation # given various values of omega, delta # Solutions for the DHO without Damping Vsol1_20 = odeint(DHO, V0, t, args=(1,20)) Vsol2_20 = odeint(DHO, V0, t, args=(2,20)) Vsol3_20 = odeint(DHO, V0, t, args=(3,20)) chisol1_20 = Vsol1_20[:,0] chisol2_20 = Vsol2_20[:,0] chisol3_20 = Vsol3_20[:,0] chisolvel1_20 = Vsol1_20[:,1] chisolvel2_20 = Vsol2_20[:,1] chisolvel3_20 = Vsol3_20[:,1] # Solutions for the DHO with Damping (wD) VsolwD1_20 = odeint(DHOwD, V0, t, args=(1,20,a,omegad)) VsolwD2_20 = odeint(DHOwD, V0, t, args=(2,20,a,omegad)) VsolwD3_20 = odeint(DHOwD, V0, t, args=(3,20,a,omegad)) chisolwD1_20 = VsolwD1_20[:,0] chisolwD2_20 = VsolwD2_20[:,0] chisolwD3_20 = VsolwD3_20[:,0] chisolwDvel1_20 = VsolwD1_20[:,1] chisolwDvel2_20 = VsolwD2_20[:,1] chisolwDvel3_20 = VsolwD3_20[:,1] # Code to plot the solution for $\\chi(t)$ on the $t$ domain. fig, (DHO, DHOwD) = plt.subplots(2,figsize=(8,6), tight_layout = True) # Code for plot WITHOUT damping DHO.plot(t,chisol1_20,label=\u0026#39;$\\omega = 20,\\delta =1$\u0026#39;) DHO.plot(t,chisol2_20,label=\u0026#39;$\\omega = 20,\\delta =2$\u0026#39;) DHO.plot(t,chisol3_20,label=\u0026#39;$\\omega = 20,\\delta =3$\u0026#39;) DHO.set_xlabel(r\u0026#39;$t$\u0026#39;, fontsize = fs) DHO.set_ylabel(\u0026#39;$\\\\chi(t)$\u0026#39;,fontsize = fs) DHO.set_title(\u0026#39;\\\\textbf{DHO w/o Driving: $(\\\\xi=1, \\\\zeta=0, a = 0)$}\u0026#39;,fontsize=fs) DHO.legend(fontsize = 12,bbox_to_anchor=(1.34,.5),loc = \u0026#34;center right\u0026#34;) # Code for plot WITH damping (wD) DHOwD.plot(t,chisolwD1_20,label=\u0026#39;$\\omega = 20,\\delta =1$\u0026#39;) DHOwD.plot(t,chisolwD2_20,label=\u0026#39;$\\omega = 20,\\delta =2$\u0026#39;) DHOwD.plot(t,chisolwD3_20,label=\u0026#39;$\\omega = 20,\\delta =3$\u0026#39;) DHOwD.set_xlabel(r\u0026#39;$t$\u0026#39;, fontsize = fs) DHOwD.set_ylabel(\u0026#39;$\\\\chi(t)$\u0026#39;,fontsize = fs) DHOwD.set_title(\u0026#39;\\\\textbf{DHO w/ Driving: $(\\\\xi=1, \\\\zeta=0, a =20)$}\u0026#39;,fontsize=fs) DHOwD.legend(fontsize = 12,bbox_to_anchor=(1.34,.5),loc = \u0026#34;center right\u0026#34;) plt.savefig(\u0026#39;DHO_test.png\u0026#39;,dpi=500) What distinguishes the two systems is the long time behavior. Whereas for the DHO without driving, all trajectories sink to a fixed point $|{\\chi(t_ \\infty)}|=0$ fixed point, the DHO with driving has a non-repetitive, aperiodic behavior at late times. In fact, we can see this behavior better by studying the phase space of solutions.\n# Plot the velocity chisolvel2_20 = Vsol2_20[:,1] chisolwDvel2_20 = VsolwD2_20[:,1] fig, (DHO_vel, DHOwD_vel) = plt.subplots(1,2,figsize=(10,5), tight_layout = True) DHO_vel.plot(chisol2_20,chisolvel2_20,label=\u0026#39;$\\omega = 20,\\delta =2$\u0026#39;) DHO_vel.set_xlabel(r\u0026#39;$t$\u0026#39;, fontsize = fs) DHO_vel.set_ylabel(\u0026#39;$\\\\chi(t)$\u0026#39;,fontsize = fs) DHO_vel.set_title(\u0026#39;\\\\textbf{DHO w/o Driving: $(\\\\xi=1, \\\\zeta=0, a = 0)$}\u0026#39;,fontsize=fs) DHO_vel.legend(fontsize = 12,loc = \u0026#34;upper right\u0026#34;) DHOwD_vel.plot(chisolwD2_20,chisolwDvel2_20,label=\u0026#39;$\\omega = 20,\\delta =2$\u0026#39;) DHOwD_vel.set_xlabel(r\u0026#39;$t$\u0026#39;, fontsize = fs) DHOwD_vel.set_ylabel(\u0026#39;$\\\\chi(t)$\u0026#39;,fontsize = fs) DHOwD_vel.set_title(\u0026#39;\\\\textbf{DHO w/ Driving: $(\\\\xi=1, \\\\zeta=0, a =20)$}\u0026#39;,fontsize=fs) DHOwD_vel.legend(fontsize = 12,loc = \u0026#34;upper right\u0026#34;) plt.savefig(\u0026#39;DHOvel_test.png\u0026#39;,dpi=500) Clearly we see that there is a fixed point attractor at $(\\chi(t_\\infty)=0,\\dot{\\chi}(t_\\infty)=0)$ when no driving is present. Whereas, for the DHO with driving, there is a circle defined by $$ \\chi(t)^2 + \\dot{\\chi}(t)^2 \\approx c^2\\ \\ \\ \\text{ for }\\ \\ \\ t\\approx t_\\infty $$ in the phase plot. Hence, the harmonic oscillator with driving and damping will never settle onto a stationary equilibrium state at late times - a classic marker of chaotic systems.\nImplementing Neural Networks # Although we were fortunate to have an exact analytic solution to this chaotic system, in general more complex chaotic systems do not admit exact solutions and novel numerical methods must be used to gain insight into the physics.\nRecently, neural networks have found success in modeling and predicting chaotic behavior. The basic idea of any neural networks is to minimize the following function $$ J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m{\\rm err}(\\theta,x_i,y_i) + f_{\\rm reg}(\\theta) $$ where $f_{\\rm reg}(\\theta)$ is a contribution used to regularize the error between the trail inputs $x_i$ with weights $\\theta$ and the outputs $y_i$ given as the training data. While the choice for the ${\\rm err}$ function is set by the regression scheme, PINN\u0026rsquo;s deviate from other neural networks by injecting physics into the regularizer term $f_{\\rm reg}$. This is done by taking the ordinary or partial differential equation describing the system (if there is one), and replacing $f_{\\rm reg}(\\theta)$ with $$ f_{\\rm reg}(\\theta; {\\rm model\\ parameters}) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\mathbb{D}({\\rm model\\ parameters})\\chi_i(\\theta,x_i)\\right)^2. $$ where $\\mathbb{D}$ is a differential operator. Then by minimizing the cost function via $\\min J(\\theta)$, one essentially teaches the neural network the physics in the problem.\nNeural Networks in Practice # Let\u0026rsquo;s see how this works in practice, I\u0026rsquo;ll return to the damped and driven harmonic oscillator with $$ \\begin{align*} {\\rm err}(\\theta,x_i,y_i) \u0026amp;= \\left(\\chi_\\text{NN}(\\theta,x_i) - y_i\\right)^2,\\newline f_{\\rm reg}(\\theta; \\delta,\\omega,a,\\omega_d) \u0026amp;= \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\left[\\frac{d^2}{dt^2}+2\\delta\\frac{d}{dt}+\\omega^2\\right]\\chi_\\text{NN}(x_i,\\theta)-a\\sin(\\omega_d t)\\right)^2. \\end{align*} $$\nThe goal now is to implement the above minimization while avoiding overfitting. My code, which is built on Ben\u0026rsquo;s, can be found on my Github here. The salient points are as follows\nI considered $10^5$ iterations on the network. The training data is taken from the domain $t\\in(0,\\frac{2}{3})$. The physics regularizer data is taken over the domain $t\\in(0,1)$. It\u0026rsquo;s important to take the regularizer data to only partly overlap the training data. If one extends the training over the full physics domain, it is not clear if the neural network is simply attempting to overfit onto the true solution due to the training data rather than learning the solution.\nMaking the Neural Network Module # Here, I outline the code for implementing neural networks in Python. First, we need to import PyTorch, set the model parameters, and choose either the DHO with or without damping.\n# Import Python\u0026#39;s Neural Network Framework import torch import torch.nn as nn # set model parameters delta = 2 omega = 20 mu, k = 2*delta, omega**2 # model choice (either chisol or chisolwD)delta_omega model_choice = chisolwD2_20 Next, we select the training data from the numerical solution. It is important to note that the ODE solver makes use of numpy whereas the neural network requires all training data as a torch.nn type and hence we need to convert between the two.\n# Note: one must convert chisolwD2 from a numpy array # to the torch nn type. x = torch.linspace(0., 3., Nt).view(-1,1) y = torch.from_numpy(model_choice).view(-1,1) # choice of sample size step_size = 20 x_data = x[0:200:step_size] y_data_temp = torch.from_numpy(model_choice[0:200:step_size]).view(-1,1) # Note: using .from_numpy converts numpy array to float64 # one must convert this torch type to float32 via: y_data = y_data_temp.to(torch.float32) Finally, we have the connected neural network code\n# The connected neural network built on the existing code # provided by Ben @ (https://github.com/benmoseley/harmonic-oscillator-pinn) def save_gif_PIL(outfile, files, fps=5, loop=0): \u0026#34;Helper function for saving GIFs\u0026#34; imgs = [Image.open(file) for file in files] imgs[0].save(fp=outfile, format=\u0026#39;GIF\u0026#39;, append_images=imgs[1:], save_all=True, duration=int(1000/fps), loop=loop) class FCN(nn.Module): def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS): super().__init__() activation = nn.Tanh self.fcs = nn.Sequential(*[ nn.Linear(N_INPUT, N_HIDDEN), activation()]) self.fch = nn.Sequential(*[ nn.Sequential(*[ nn.Linear(N_HIDDEN, N_HIDDEN), activation()]) for _ in range(N_LAYERS-1)]) self.fce = nn.Linear(N_HIDDEN, N_OUTPUT) def forward(self, x): x = self.fcs(x) x = self.fch(x) x = self.fce(x) return x # A simple plotting function for data visualization def plot_result(x,y,x_data,y_data,yh,xp): plt.figure(figsize=(10,5)) plt.plot(x, y, color=\u0026#34;grey\u0026#34;, linewidth=2, alpha=0.8, label=\u0026#39;$\\omega = 20,\\delta =2$\u0026#39;) plt.plot(x, yh, color=\u0026#34;tab:blue\u0026#34;, linewidth=4, alpha=0.8, label=\u0026#34;Neural network prediction\u0026#34;) plt.scatter(x_data, y_data, s=60, color=\u0026#34;tab:orange\u0026#34;, alpha=0.4, label=\u0026#39;Training data\u0026#39;) if xp is not None: plt.scatter(xp, -0*torch.ones_like(xp), s=60, color=\u0026#34;tab:green\u0026#34;, alpha=0.4, label=\u0026#39;Physics loss training locations\u0026#39;) l = plt.legend(bbox_to_anchor=(1.36,.5),loc = \u0026#34;center right\u0026#34;,fontsize=12) plt.setp(l.get_texts(), color=\u0026#34;k\u0026#34;) plt.title(\u0026#39;\\\\textbf{DHO w Driving: $(\\\\xi=1, \\\\zeta=0, a = 0)$}\\hspace{.2in}\u0026#39;\u0026#34;Training step: %i\u0026#34;%(i+1),fontsize=fs) plt.xlabel(r\u0026#39;$t$\u0026#39;,fontsize=fs) plt.ylabel(\u0026#39;$\\\\chi(t)$\u0026#39;,fontsize=fs) # sample locations over the problem domain x_physics = torch.linspace(0., 1., 30).view(-1,1).requires_grad_(True) torch.manual_seed(123) model = FCN(1,1,32,3) optimizer = torch.optim.Adam(model.parameters(),lr=1e-4) files = [] for i in range(100000): optimizer.zero_grad() # compute the \u0026#34;data loss\u0026#34; yh = model(x_data) loss1 = torch.mean((yh-y_data)**2)# use mean squared error # compute the \u0026#34;physics loss\u0026#34; yhp = model(x_physics) dx = torch.autograd.grad(yhp, x_physics, torch.ones_like(yhp), create_graph=True)[0]# computes dy/dx dx2 = torch.autograd.grad(dx, x_physics, torch.ones_like(dx), create_graph=True)[0]# computes d^2y/dx^2 # computes the residual of the 1D harmonic oscillator differential equation physics = dx2 + mu*dx + k*yhp - a*torch.sin(omegad * x_physics) loss2 = (1e-4)*torch.mean(physics**2) # backpropagate joint loss loss = loss1 + loss2# add two loss terms together loss.backward() optimizer.step() # plot the result as training progresses if (i+1) % 150 == 0: yh = model(x).detach() xp = x_physics.detach() plot_result(x,y,x_data,y_data,yh,xp) file = \u0026#34;plots/pinn_%.8i.png\u0026#34;%(i+1) plt.savefig(file, bbox_inches=\u0026#39;tight\u0026#39;, pad_inches=0.1, dpi=100, facecolor=\u0026#34;white\u0026#34;) files.append(file) if (i+1) % 900 == 0: plt.show() print(loss1,loss2) else: plt.close(\u0026#34;all\u0026#34;) save_gif_PIL(\u0026#34;chaospinn.gif\u0026#34;, files, fps=20, loop=0) Here\u0026rsquo;s how well the NN performs on the DHO with driving:\nThe NN predicts the solution over the domain $t\\in(0,1)$ which is well beyond the training data. Outside the domain, however, the NN fails to understand the chaotic behavior and is the prediction comparable to an ordinary neural network.\nEssentially, this occurs since $f_{\\rm reg}$ is only defined in the domain $t\\in(0,1)$ suggesting that the neural network truly requires the physics knowledge over the full domain to provide a better prediction. Let\u0026rsquo;s see what happens if we provide more training data over a larger domain $t\\in(0,2)$. I find this beautiful behavior\nClearly, the NN appears to do better when extending $f_{\\rm reg}$ on a larger domain. One downside, however, is that this is computationally expensive. Performing $10^{5}$ iterations took me around 5 minutes to complete on my mid 2015 MBP. Ideally, PINN\u0026rsquo;s should be implemented on systems capable of handling MPI or multithreading to reduce the computational cost.\nConclusions # It is possible to learn chaos with neural networks by implementing the PINN architecture. Although I studied this problem for a toy model, I suspect that one can extend this procedure to more complicated, realistic systems. However, it should be noted that the computation time scales with the system\u0026rsquo;s complexity (i.e., for PDEs rather than ODEs, or for larger time domains, etc.). Hence, there is an art to implementing PINNs in general and this architecture should be supplemented with additional algorithms to reduce computational time.\nS. H. Strogatz, Nonlinear Dynamics $\\textit{\\\u0026amp;}$​ Chaos.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"8 May 2022","permalink":"/posts/2022-05-04-learning-chaos-with-nn-ii/","section":"Posts","summary":"I have a soft spot for studying chaotic systems.","title":"Learning Chaos With Neural Networks II"},{"content":" I am an NSF Graduate Research Fellow and a DDF Fellow at the University of Minnesota.\nPreviously, I completed Part III of the Mathematical Tripos at the University of Cambridge during 2016-2017.\nBefore starting my Ph.D, I was a vistor at the Perimeter Institute for Theoretical Physics in the Afshordi group from 2017-2018.\nI am interested in theoretical aspects of gravity; particularly on understanding the classical, semiclassical, and quantum properties of black holes. Some of my projects are highlighted below.\nMy publications can be found here. For more information, please see my CV.\nQuantum Gravity $\\textit{\\\u0026}\\ $ Holography I have a long history of being involved in studying aspects of quantum gravity via holography. Holography, also referred to as AdS$_{d+1}$/CFT$_{d}$ or as the gauge/gravity duality, is a collection of ideas which form a toolkit. The toolkit allows one to convert difficult problems in a type of quantum field theory, known as conformal field theory (CFT$_{d}$), into simpler problems encountered by gravity theories with a negative cosmological constant, called anti de Sitter gravity (AdS$_{d+1}$). One condition to use the toolkit is that the conformal field theory operates in $d$-dimensions whereas the gravity theory lives in one dimension higher, i.e., $d+1$-dimensions. Earlier in my academic career, I used gravity to model aspects of nuclear physics and the theory of strong interactions, QCD -- which behaves like a CFT$_d$ at high energies. Recently, I have turned my attention to using CFT$_d$ to answer problems in gravity through the holographic toolkit. Along this line, my current research interests are entanglement entropy, lower dimensional gravity, and black hole information. Deforming CFT$_d$ and the Hawking-Page Transition\nIn my most recent paper on holography, my collaborator and I established a new result on the Hawking-Page transition in AdS$_{d+1}$ gravity and its relationship to the deconfinement/confinement phase transition found in certain CFT$_{d}$. Like the phase transition where ice melts into water at high temperatures, composite particles, such as protons and neutrons, also melt at high temperatures into a quark-gluon plasma. When a phase transition occurs in the CFT$_d$ side, holography tells us that a black hole is expected to spontaneously form from a collapse of thermal gas in the AdS$_{d+1}$ side. The spontaneous formation of black holes at a critical temperature $\\beta$ is known as the Hawking-Page transition. In an earlier paper by Edward Witten , it was observed that the deconfinement/confinement phase transition can be avoided by deforming the CFT$_d$ side. In such cases, it was conjectured that black hole formation is prohibited in such cases. We asked a simple question; Is this conjecture true, and if so, when this phase transition is avoided, is the formation of black holes suppressed abruptly or continuously? This is a sharp quantum gravity question we can answer via the holographic duality. Our construction uses a parameter $\\theta$ which controls the phase transition. The results are highlighted in the right figure. We found that the phase transition (the filled regions) is avoided at $\\theta=\\pi/3,\\pi,5\\pi/3$ whereas the black hole formation (the hatched region) is prevented whenever $\\theta\\in[\\pi/3,5\\pi/3]$. The result is surprising for several reasons. We expected black hole formation to be suppressed abruptly but it appears that black holes can still form for a small window! Secondly, there are mismatchs - there are missing islands and the temperatures $\\beta$ do not precisely match up at $\\theta=0$. It is always exciting to encounter such mismatches as they inspire deeper questions about the nature of gravity in holography that I am currently investigating! Uniform Black String Instabilities in Anti de Sitter In 2021, my collaborator and I showed that uniform black strings in AdS (AdS UBS) suffer from the Gregory-Laflamme instability; an instability which causes black string objects to develop a pinch in their horizons (shown in the left cartoon). At the end state of the instability, a singularity is expected to appear and the black string fragments into two black holes. It was previously suspected that the AdS UBS could avoid the instability. However, we performed a numerical search for the instability and found a parameter regime where the instability can occur (shown in the below images in $d=5,\\dots,9$ on the left and $d=10$ on the right). If the conditions are just right, a black string will fragment into several smaller black holes at the end state of the GL instability. The transition between uniform black strings and black holes (BH) are interesting as a naked singularity is formed in finite time. However, if certain conditions are not met, the BS will instead converge onto different geometry without a naked singularity forming. In this paper, we argued that the AdS UBS could either fragment into a BH or converge onto a black funnel geometry (BF). We found conditions where the fragmentation is more likely (i.e., entropically favorable) rather transitioning smoothly to a BF state. ","date":"27 April 2022","permalink":"/about/","section":"Hi! I'm Aditya","summary":"I am an NSF Graduate Research Fellow and a DDF Fellow at the University of Minnesota.","title":"👋🏽 Welcome!"},{"content":" I\u0026rsquo;ve been really interested in machine learning and neural networks in the context of physics recently. When I came across Ben Moseley\u0026rsquo;s post on Physics Informed Neural Networks, I instantly knew what I was going to be doing this afternoon \u0026ndash; I wanted to implement neural networks using Mathematica to solve a simple problem, the Harmonic Oscillator. As Sidney Coleman once said,\n\u0026ldquo;The career of a young theoretical physicist consists of treating the harmonic oscillator in ever-increasing levels of abstraction,\u0026rdquo;\nand applying sufficiently well trained neural network to a canonical problem in physics is well within the limits of abstraction.\nTheory # The canonical problem of the harmonic oscillator is to understand a system with a restoring force \\(F(\\chi)=-k\\chi.\\) For a single restoring force acting on masses connected to springs, or pendulums freely moving in a gravitational field, the system undergoes simple harmonic motion induced by the restoring force. Mathematically, the objective is to solve the system:\n$$ \\begin{equation*}m\\frac{d^2\\chi(t)}{dt^2} + k\\chi(t)=0\\end{equation*} $$\nsubject to some initial data \\((\\chi(0)=\\xi,\\chi\u0026rsquo;(0)=\\zeta)\\). Of course, any budding physicist knows there is an exact solution\n$$\\begin{equation*}\\chi(t) = \\frac{\\zeta}{\\omega}\\sin(\\omega t) + \\xi\\cos(\\omega t)\\end{equation*}$$\nwhich has a straightforward wave-like behavior plotted below for a representative set of solutions.\nApplications # Now, we turn to the problem of constructing neural networks with Mathematica architecture in order to learn harmonic motion directly. We will train the network for the parameters and the domain\n$$\\begin{equation*}[\\xi=1,\\zeta=0,\\omega=3;\\ t_{\\rm sample}\\in(0,\\pi)]\\end{equation*}$$\nMathematica\u0026rsquo;s neural network algorithms can produce a trained fit to the initial sample. Here, I show how I did it but more documentation can be found in this post.\nThe algorithm is as follows. First, construct the solution with the parameters above. I created a function Sols which solves the differential equation for harmonic motion provided data $(\\xi,\\zeta,\\omega)$. For this particular example, I created an instance\nOscillator[t_] := Evaluate[Sols[t, 1, 0, 3]]; Next, gather some training data. I chose about 30 points to train the neural network (NN).\ndata = Simplify[ Table[t -\u0026gt; Oscillator[t], {t, 0, \\[Pi], \\[Pi]/30}] // N]; Now, construct a neural network with NetChain. I chose a NN with 5 hidden layers, and 1 input/output layer respectively. The model uses different types of ElementwiseLayer, such as IntegerParts, Tanh, and LogisticSigmoid, to train the network. I found that this was best for convergence.\nnet = NetChain[{150, Tanh, 150, LogisticSigmoid, 1}] The five entries provided in NetChain define functions only on the hidden layer. Finally, we can train the model with NetTrain.\ntrainingdata = NetTrain[net, data, All, TimeGoal -\u0026gt; 30] Essentially, this takes the NN and trains it on the sample data in around 30 seconds. The NN converged rather quickly to a loss of about $3.67\\times 10^{-5}$.\nAnd we\u0026rsquo;re done! Let\u0026rsquo;s visualize the trained data now, the results are shown in the plot below.\nAlthough the neural networks performs a good fit when trained, it has not learned any physics in any meaningful way. So for now, it seems a young theoretical physicist can still beat a well trained neural network. But this result brings up a natural question, can we do better using Mathematica architecture?\nIt turns out the answer is no, see here for a more extensive discussion.\nThe goal of Part II is to reproduce Ben\u0026rsquo;s calculation in Physics-Informed Neural Networks by writing better neural networks which incorporates a regularization scheme where the equation of motion is simultaneously minimized along with a cost function.\n","date":"20 April 2022","permalink":"/posts/2022-04-20-physical-neural-networks---the-harmonic-oscillator/","section":"Posts","summary":"I\u0026rsquo;ve been really interested in machine learning and neural networks in the context of physics recently.","title":"Physical Neural Networks - The Harmonic Oscillator, Part I"},{"content":" The entanglement entropy is a measure of how quantum information is stored in a quantum state. In particular, the entanglement entropy measures the amount of entanglement between two subsystems of a composite quantum system. Here, I will review the entanglement entropy and then compute it for a simple example \u0026ndash; the two spin system both analytically and with the QuantumFramework Packlet on Mathematica 13. I will largely be following Nishioka 2018, Hong Liu\u0026rsquo;s HEE MIT lecture, and Tom Hartman\u0026rsquo;s excellent notes.\nTheory # The canonical example is to consider a system which is subdivided into two regions $A$ and $B$, see figure below taken from Nishioka 2018.\nThe system $\\Sigma$ is said to be bipartite if its Hilbert space $\\mathcal{H}_ \\Sigma$ can be expressed by the tensor product $$ \\mathcal{H}_ \\Sigma = \\mathcal{H}_ I\\otimes\\mathcal{H}_ {I^c} $$ over the individual subsystems where $I=A$ or $B$. To see if the two subsystems are entangled, we need further technologies: a density matrix $\\rho_ \\Sigma$ and an operation $\\text{Tr}_ I$ which isolates a subsystem. The first is easy, the density matrix, by definition, is constructed from the (pure) quantum state $\\ket{\\Psi}\\in\\mathcal{H}_ \\Sigma$ of the system\n$$\\begin{equation*}\\rho_ \\Sigma = \\ket{\\Psi}\\bra{\\Psi}.\\end{equation*}$$\nFor the second, let ${\\ket{i}_ I, i=1,2,\\cdots, \\dim(\\mathcal{H}_ I)}$ be an orthonormal basis for $\\mathcal{H}_ I$. The partial trace $ \\text{Tr}_ {I}$ of an operator $\\mathcal{O}$ is then defined by\n$$\\begin{equation*}\\text{Tr}_ I(\\mathcal{O}) \\equiv \\sum_ {i} {}_ {I}\\bra{i}\\mathcal{O}\\ket{i}_ {I}.\\end{equation*}$$\nThe partial trace can be used to isolate a subsystem $I$ by \u0026ldquo;tracing out\u0026rdquo; the complement $I^c$. To be concrete, the reduced density matrix of the subsystem $I$ is obtained via\n$$\\begin{equation*}\\rho_ {I} \\equiv \\text{Tr}_ {I^c}(\\rho_ \\Sigma).\\end{equation*}$$\nWe now have the necessary ingredients to compute the von Neumann entropy stored in the entanglement between $I$ and $I^c$. The entanglement entropy of the subsystem $I$ is computed by the formula\n$$\\begin{equation*}S_ I = - \\text{Tr}_ {I}[\\rho_ {I}\\log(\\rho_ {I})].\\end{equation*}$$\nThis entropy satisfies the following statements:\nThe entanglement entropy is non-zero only if the complement of the subsystem is non-empty. In other words, $S_ I=0$ when $\\text{Area}(I^c)=0$. To see this, first note that $\\text{Area}(I^c)=0$ is the same as saying $I^c=\\emptyset$. Next, we have $\\Sigma = I \\cup I^c = I \\cup \\emptyset = I$. And finally, it is well known that the entanglement entropy of the total system always vanishes for a pure state. Proof: $$\\begin{align*}S_ I \u0026amp;= - \\text{Tr}_ {I}[\\rho_ {I}\\log(\\rho_ {I})] = - \\text{Tr}_ {\\Sigma}[\\rho_ \\Sigma\\log(\\rho_ \\Sigma)] = - \\text{Tr}_ {\\Sigma}[ODO^{-1}\\log(ODO^{-1})]\\newline\u0026amp;=-\\text{Tr}_ {\\Sigma}[D\\log(D)]=0\\end{align*}$$ where we have used that the density matrix (i) can be diagonalized, i.e., $$\\rho_{\\Sigma}=ODO^{-1}$$ and (ii) the ground state is pure so $$\\rho_\\Sigma = (\\rho_\\Sigma)^2$$ implying that the eigenvalues $\\lambda_i\\in{0,1}.$\nThe entanglement entropy is finite for finite-dimensional quantum systems, such as on a lattice. However, $S_I$ suffers from UV divergences in quantum field theory (necessarily infinite dimensional) due to short range interactions near the boundary of $I$, denoted $\\partial I$.\nIt is possible to show that the entanglement entropy vanishes only if the pure ground state is separable which means that the quantum state admits the decomposition $\\ket{\\Psi} = \\ket{\\Psi}_ I\\otimes\\ket{\\Psi}_ {I^c}$. In the case where $\\text{Area}(I^c)=0$, there exists no states in $\\mathcal{H}_ {I^c}$ and hence the bipartition is simple $\\ket{\\Psi}=\\ket{\\Psi}_ I\\otimes 1 = \\ket{\\Psi}_ \\Sigma$.\nApplications # This is all well and good in theory, but how does one actually go about computing the entanglement entropy given a state?\nThe simplest example is the two spin system (see Fig. 1(a)). Let $A$ and $B$ be two particles with spin $s=1/2$. Each particle has two states in their respective Hilbert spaces $$H_ {I}=\\text{span}({\\ket{\\uparrow}_ I,\\ket{\\downarrow}_ I})$$ where $I=A$ or $B$ and the bases are orthonormal $\\braket{i}{j}=\\delta_{ij}$ for $i,j\\in{\\uparrow,\\downarrow}$. The total Hilbert space is then straightforward to build $$\\mathcal{H}_ \\Sigma =\\mathcal{H}_ I\\otimes\\mathcal{H}_ {I^c}= {\\ket{\\downarrow\\downarrow},\\ket{\\downarrow\\uparrow},\\ket{\\uparrow\\downarrow},\\ket{\\uparrow\\uparrow}}$$ where $\\ket{ij}\\equiv\\ket{i}_ I\\otimes\\ket{j}_ {I^c}$.\nNow, suppose the ground state is given as\n$$\\begin{equation*}\\ket{\\Psi} = \\frac{1}{\\sqrt{2}}(\\ket{\\downarrow\\uparrow}+\\ket{\\uparrow\\downarrow})\\end{equation*}$$\nthen the density matrix of the total system reads\n$$\\begin{equation*}\\rho_\\Sigma=\\ket{\\Psi}\\bra{\\Psi}=\\frac{1}{2}(\\ket{\\downarrow\\uparrow}+\\ket{\\uparrow\\downarrow})^2.\\end{equation*}$$\nTracing out, say $I^c=B$, we arrive at the reduced density matrix for $I=A$\n$$\\begin{align*}\\rho_A = \\frac{1}{2}(\\ket{\\downarrow}_A{}_A\\bra{\\downarrow}+\\ket{\\uparrow}_A{}_A\\bra{\\uparrow}) = \\begin{pmatrix}1/2 \u0026amp; 0\\newline0 \u0026amp;1/2\\end{pmatrix} \\end{align*}$$\nAnd now a straightforward calculation of the entanglement entropy $S_A$ shows\n$$\\begin{align*}S_{A} \u0026amp;= -\\text{Tr}_A\\left[\\begin{pmatrix}1/2 \u0026amp; 0\\newline0 \u0026amp;1/2\\end{pmatrix}\\cdot\\begin{pmatrix}\\log(1/2) \u0026amp; 0\\newline0 \u0026amp;\\log(1/2)\\end{pmatrix}\\right]\\newline\u0026amp;=\\log(2)\\end{align*}$$\nThis seemingly innocous value is in fact the maximum amount of von Neumann entropy (or more loosely information) stored between two entangled particles! To prove this statement, let us consider a more general state\n$$\\ket{\\Psi} = \\cos(\\theta)\\ket{\\downarrow\\uparrow}-\\sin(\\theta)\\ket{\\uparrow\\downarrow}$$\nand study this problem in Mathematica 13.\nEntanglement Entropy on Mathematica 13 # Recently, the Quantum Information Research Team at Wolfram Alpha have developed frameworks which greatly simplify the calculations required to compute the entanglement entropy (in low dimensional examples). This code can be found on my Github.\nThe code snippet below demonstrates how the QuantumFramework packlet can be used to reproduce the entanglement entropy of the Bell state.\nNow to study the generalized Bell state, we essentially reproduce the code but have to change the initial state:\nHence, the entanglement entropy stored in the generalized entangled states is\n$$S_A(\\theta) = -2\\left[\\cos(\\theta)^2\\log(\\cos(\\theta))+\\sin(\\theta)^2\\log(\\sin(\\theta)) \\right]$$\nNow, we are ready to maximize $S_A(\\theta)$ and prove that $S_A=\\log(2)$ is the maximum amount of entropy which can be stored in two-spin system. This amounts to finding $\\theta_c$ such that $$\\frac{d}{d\\theta}S_A(\\theta)\\rvert_{\\theta_c}=0.$$ A quick calculation shows\n$$\\theta_c = \\frac{\\pi}{4} \\implies S_{A}(\\theta_c) = \\log(2).$$\nIt is also immediate to show this on Mathematica:\nThe purpose of this post was to discuss some basic facts about the entropy of entanglement and study a simple example both analyticially and numerically. In a future post, we study the case of the Thermofield double state as we slowly build towards the holographic entanglement entropy.\nAcknowledgments # I appreciate Å. Folkestad for comments.\n","date":"7 February 2022","permalink":"/posts/2022-02-07-entanglement-entropy/","section":"Posts","summary":"The entanglement entropy is a measure of how quantum information is stored in a quantum state.","title":"Computing the Entanglement Entropy with the Quantum Framework Paclet"},{"content":"The conceptually cleanest way to understand the thermodynamics of black holes is by studying the Euclidean path integral for gravity. This integral, denoted by $\\mathcal{Z}$, is defined as follows\n$$\\begin{equation}\\mathcal{Z} = A \\int[\\mathcal{D}g] e^{-\\mathcal{I}[g]}.\\end{equation}$$\nThis is a functional integral with a normalization A, measure $\\mathcal{D}[g]$, and integrand $e^{-\\mathcal{I}[g]}$ of metrics $g$ with Euclidean signature. (A primer on functional integration will be released shortly.) Essentially, computing the path integral for any system, subject to certain conditions discussed below, allows one to completely characterize the thermodynamics of the system. In practice, the above path integral is impossible to evaluate directly except in special cases. Rather than evaluating $\\mathcal{Z}$ exactly, there are several techniques to approximate its value. One such technique is the saddle point approximation. The idea is that when the action $\\mathcal{I}[g]\\gg1$, the integrand $e^{\\mathcal{I}[g]}\\ll1$. Hence, the most dominant contributions are given by the minima of the function where we can approximate\n$$\\mathcal{I}[g] = \\sum_i\\mathcal{I}[g_ i] + \\frac{1}{2}(g-g_ i)^2\\frac{\\delta^2}{\\delta g^2}\\mathcal{I}[g]|_ {g=g_i}+\\dots$$\nThen the path integral simply reduces to a Gaussian integral which can be normalized as follows\n$$\\begin{equation}\\mathcal{Z} \\approx A e^{-\\sum_{i}\\mathcal{I}[g_ i]} \\int[\\mathcal{D}g]e^{\\sum_ i \\frac{1}{2}(g-g_ i)^2\\frac{\\delta^2}{\\delta g^2}\\mathcal{I}[g]|_ {g=g_ i}+\\dots} = e^{-\\sum_{i}\\mathcal{I}[g_i]}.\\end{equation}$$\nThus the entire contribution of the geometry to the Euclidean path integral is encoded in the minima of the action functional.\nIn order to make contact with thermodynamics, we invoke the following equivalence: the action in Euclidean signature is proportional to the free energy of the system with the proportionality set by the inverse temperature, i.e., $\\mathcal{I}[g] = \\beta F = (1/T)F$.\nTo be concrete, I detail the thermodynamics for Euclidean gravity with a negative cosmological constant, or Einstein-AdS gravity. This theory of gravity admits two minima: a Schwarzschild-AdS black hole, and a thermal gas of photons in AdS spacetime. These two minima admit phase transitions at a critical tempeture which will be studied in a later article.\nGravitational Saddle Points in AdS$_d$ # We will review the purely gravitational saddle points allowed in AdS$_ {d+2}$ space and describe the Hawking-Page phase transition between a thermal AdS background and an SAdS$_ {d+2}$ black hole. Let $\\mathcal{M}$ be a Lorentzian manifold and $\\partial\\mathcal{M}$ a codimension one boundary. We consider the theory\n$$\\begin{equation} \\mathcal{I} = -\\frac{1}{8\\pi G_ {d+2}} \\times\\left[\\mathcal{S}_ {\\mathcal{X}}+\\mathcal{S}_ {\\partial\\mathcal{X}}\\right]\\end{equation}$$\nwhere the bulk and the boundary actions are respectively\n$$\\begin{align} \\mathcal{S}_ \\mathcal{X} =\u0026amp; \\frac{1}{2}\\int_ \\mathcal{X}\\text{d}^{d+2}x\\sqrt{-g} \\left[R - 2\\Lambda\\right],\\newline \\mathcal{S}_ {\\partial\\mathcal{X}} =\u0026amp; \\int_{\\partial\\mathcal{X}}\\text{d}^{d+1}x\\sqrt{-h}\\left[\\mathcal{K}+\\frac{d+1}{\\ell^2} + c_1(d) \\mathcal{R} \\right.\\newline \u0026amp;+\\left.c_2(d)\\left(\\mathcal{R}_ {ab}\\mathcal{R}^{ab} - c_3(d)\\mathcal{R}^2\\right) +\\cdots \\right]\\nonumber. \\end{align}$$\nThe boundary action is necessary for a well-posed variational principle and to regularize the bulk action. Here $\\Lambda = -d(d+1)/2\\ell^2$, $R(g)$ is the scalar curvature, and $\\mathcal{K}$ and $\\mathcal{R}$ are the extrinsic and scalar curvatures of the boundary metric $h$ respectively. Note that higher curvature corrections to the boundary action appear in $d\\ge 6$ and the coefficients $c_i(d)$ are sensitive to the dimension and the topology of the horizon. The equations of motion and the trace of the Einstein equations read\n$$\\begin{align*} R_{ab} - \\frac{1}{2}g_{ab}R + \\Lambda g_{ab} \u0026amp;= 0\\newline \\frac{d}{2}R + (d+2)\\Lambda \u0026amp;=0 \\end{align*}$$\nfrom which the on-shell bulk action then simplifies considerably to a single volume integral\n$$ \\begin{equation} \\mathcal{S}_ {\\mathcal{X}} = \\frac{1}{2}\\int_{\\mathcal{X}}\\text{d}^{d+2}x\\sqrt{-g}\\left(\\frac{4\\Lambda}{d}\\right)= \\frac{2\\Lambda}{d} \\text{Vol}(\\mathcal{X}). \\end{equation}$$\nHence, the entire contribution of the bulk action, $\\mathcal{I}$, to the thermodynamics is only dependent on the topological nature of the transverse directions to the horizon. Clearly, the bulk action is divergent since we are integrating over the entire space. The boundary action $\\mathcal{S}_ {\\partial\\mathcal{X}}$ regulates this divergence rendering $\\mathcal{I}$ finite.\nBlack Hole Saddles # Let us apply this procedure to saddle points of the above action. First, we have to identify geometries which are regular after Wick rotation $\\tau = i t$ to Euclidean space. The general solution, in static coordinates, is given by\n$$\\begin{align} \\text{d} s^{2}\u0026amp;=-f(r)\\text{d} t^{2}+\\frac{\\text{d} r^{2}}{f(r)}+\\frac{r^{2}}{\\ell^2}\\text{d}\\Sigma_{d,k}^{2} \\newline f(r)\u0026amp;=k-\\frac{m}{r^{(d-1)}}+\\frac{r^{2}}{\\ell ^{2}}. \\end{align}$$\nThere are two relevant saddles; the Schwarzschild-AdS$_ {d+2}$ solution, $m \u0026gt; 0$, and the pure AdS$_ {d+2}$, $m=0$, solution. Here, the $d$-dimensional metric $\\text{d}\\Sigma_{d,k}$ corresponds to the various boundary geometries which respect the isometries of AdS$_ {d+2}$ spacetime. In particular,\n$$\\begin{equation} \\text{d}\\Sigma_{d,k}^2 = \\begin{cases} \\ell^2\\text{d}\\Omega_d^2 \u0026amp;\\text{for}\\hspace{.1in} k=1,\\newline \\sum\\limits_{i=1}^d\\text{d} x_i^2 \u0026amp;\\text{for}\\hspace{.1in} k=0,\\newline \\ell^2\\text{d}\\Xi_d^2 \u0026amp;\\text{for}\\hspace{.1in} k=-1, \\end{cases} \\end{equation}$$\nwhere $\\text{d}\\Omega_d^2$ and $\\text{d}\\Xi_d^2$ are the unit metrics on $S^d$ and $H^d$. Thus $k=+1,0,-1$ represents spherical, planar, and hyperbolic boundaries respectively. We denote the outer horizon by $r_+$ which is located at the largest positive root of $f(r_+)=0$.\nBlack Hole Thermodynamics # To compute the thermodynamics, we perform an analytic continuation of time into the Euclidean section. In order to avoid a conical singularity, we must have $$\\tau \\sim \\tau + {4\\pi}/{f\u0026rsquo;(r_+)}.$$ The temperature may be computed via the standard Euclidean trick\n$$\\begin{align} T(r_+)\u0026amp;=\\frac{f^\\prime(r_+)}{4\\pi}=\\frac{(d-1)k}{4 \\pi r_+} +\\frac{(d+1) r_+}{4 \\pi \\ell ^2} \\end{align}$$\nThe remaining thermodynamic quantities are\n$$\\begin{align} \\mathcal{I}^* \u0026amp;= \\beta \\frac{\\sigma_{d,k}}{16\\pi G_{d+2}} \\left[r^{d-1}\\left(k - \\frac{r^{2}}{\\ell^2}\\right)\\right] - \\mathcal{I}_ {k}^0, \\newline E \u0026amp;= \\partial_\\beta{\\mathcal{I}} = \\frac{d,\\sigma_{d,k}}{16\\pi G_{d+2}}m - E_{k}^0,\\newline S \u0026amp;= \\frac{A(\\mathcal{H})}{4G_{d+2}} = \\frac{\\sigma_{d}r^{d}_ +}{4G_{d+2}} \\end{align}$$\nHence, the parameter $m$ appearing in the metric function is related to the ADM mass of the black hole. Here, $\\sigma_d$ is the volume of the unit $d$-dimensional boundary. Note that we may solve for $r_+(T)$ from $T(r_+)$.\nFor $k=+1,0$, the thermodynamics and the on-shell Euclidean action can either be calculated with the usual counterterm method or through a background subtraction technique. The background subtraction identifies the locally AdS$_ d$ space, i.e., $m=0$, as the reference ground state in which case $r_ {+,k}=0$. However, with $k=-1$, the locally AdS$_ d$ solution contains a bifurcate Killing horizon at $$r_c=\\ell$$ at a fixed temperature. In this case, performing either a background subtraction or utilizing the counterterm method introduces a conical singularities. These singularities are removed when one subtracts an extremal saddle leading to the corrections $\\mathcal{I}_ k^0$.\nThe classical and thermodynamic stability of these solutions have been rigorously explored in here, here, and here. The heat capacity for these solutions is\n$$\\begin{equation} C = \\frac{d\\sigma_{d,k}}{4G_{d+2}}r_+^d\\left(\\frac{(d+1)r_+^2 + (d-1)k\\ell^2}{(d+1)r_+^2 - (d-1) k\\ell^2}\\right). \\end{equation}$$\nWe see that, for non-positive $k$, the heat capacity is always non-negative indicating planar, and hyperbolic black holes are thermodynamically stable. For $k=-1$, the heat capacity vanishes precisely when $r_{+,c} = \\sqrt{(d-1)/(d+1)}\\ell.$\n","date":"3 November 2021","permalink":"/posts/2021-11-03-black-hole-thermodynamics/","section":"Posts","summary":"The conceptually cleanest way to understand the thermodynamics of black holes is by studying the Euclidean path integral for gravity.","title":"Black Hole Thermodynamics in AdS$_ d$"}]